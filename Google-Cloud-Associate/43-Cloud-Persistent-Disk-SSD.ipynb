{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Persistent Disks and Local SSDs<br>\n",
    "\n",
    "***\n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Block Storage**<br>\n",
    "\n",
    "by default, each Compute Engine instances have a single boot persistent disks that contains OS<br>\n",
    "you can add more persistent disks or SSD to your instance<br>\n",
    "\n",
    "- **Persistent Disk (pd)**<br>\n",
    "\n",
    "Option 1: Zonal - Regional<br>\n",
    "mixed with<br>\n",
    "Option 2: Standard - Balanced - SSD<br>\n",
    "\n",
    "Persistent disks are durable network storage devices, that your instances can access.<br>\n",
    "They're not physically attached disks, but network disks that are connected in Google's internal network<br>\n",
    "\n",
    "Persistent disks are independent, it can persist after instances has been terminated<br>\n",
    "you can also detach the disk and move it to other instances if you need to<br>\n",
    "\n",
    "disk resize feature allows you to rescale, with no downtime and add additional disks<br>\n",
    "They're encrypted by default<br>\n",
    "\n",
    "pd can be up to 64TB in size<br>\n",
    "most instances can have up to 128 persistent disks and 257TB total persistent disks space attached<br>\n",
    "Shared core machine types are limited to 16 persistent disks and 3TB total persistent disk space<br>\n",
    "\n",
    "Option 1 is picking the geographic options.<br>\n",
    "Zonal - disks that are only available in 1 zone, 1 region<br>\n",
    "most commonly used disks for day to day usage<br>\n",
    "they can't survive an outage of that zone, and can be affected by data loss<br>\n",
    "this is where snapshot comes into play<br>\n",
    "Zonal disks can work with any machine types, including pre-defined, shared-core, custom machine type<br>\n",
    "\n",
    "Regional - similar qualities to zonal, but<br>\n",
    "regional provides durable storage, replication of data in 2 zones of same region<br>\n",
    "if you're designing a system that uses high compute engine, you need to use this<br>\n",
    "also designed to work with regional managed instance groups<br>\n",
    "\n",
    "regional disks are slower than zonal disks<br>\n",
    "so consider it if write performance is less critical and data availability across the zones<br>\n",
    "regional standard disks have 200GB size minimum requirement<br>\n",
    "and regional disks can't work with memory/compute optimized machine types<br>\n",
    "<br>\n",
    "\n",
    "**pd-standard**<br>\n",
    "\n",
    "backed by standard hard disk drives(HDD)<br>\n",
    "large data processing workloads that primarily use sequential I/Os<br>\n",
    "lowest pricd persistent disks<br>\n",
    "<br>\n",
    "\n",
    "**pd-balanced**<br>\n",
    "\n",
    "alternative to SSD persistent disks, balanced performance and cost<br>\n",
    "same maximum IOPS and ssd-pd, lower IOPS per GB<br>\n",
    "general purpose<br>\n",
    "price in between standard and SSD persistent disks<br>\n",
    "\n",
    "about 3x faster than pd-standard in performance at max IOPS<br>\n",
    "<br>\n",
    "\n",
    "**pd-ssd**<br>\n",
    "\n",
    "fastests persistent disks<br>\n",
    "enterprise applications and high-performance databases that demand lower latency and more IOPS<br>\n",
    "single-digit ms latencies<br>\n",
    "highest priced persistent disks<br>\n",
    "\n",
    "about 5x faster than pd-balanced in max IOPS<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Local SSD**<br>\n",
    "\n",
    "Option 1: SCSI - NVME<br>\n",
    "Option 2: Local SSD<br>\n",
    "\n",
    "physically attached to your server that hosts local instances<br>\n",
    "local ssds have higher throughput/lower latency than pd<br>\n",
    "it's because it's physically attached and data doesn't have to travel over the network<br>\n",
    "\n",
    "data persists until instance is stopped or deleted<br>\n",
    "each local ssd 375GB, attach max of 24 local ssd for total of 9TB per instance<br>\n",
    "fast scratch disk or cache<br>\n",
    "\n",
    "SCSI - older protocol, made for harddrives and hold limitation of 1 queues per command<br>\n",
    "Non violatile memory express (NVME) - is a newer protocol, use flash memory, designed upto 64k queues<br>\n",
    "only available for n1,n2 and compute optimized machine types<br>\n",
    "\n",
    "about 10-25x faster than pd-ssd in max IOPS<br>\n",
    "<br>\n",
    "\n",
    "performance scaling in block storage in compute engine<br>\n",
    "\n",
    "pd performance scales with the size of disk, and number of vCPUs<br>\n",
    "pd performance scales linearly until the limits of volume or limits of each compute instance<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo**<br>\n",
    "\n",
    "this demo is on how to manage and interface with disks in Compute Engine<br>\n",
    "\n",
    "**creating an instance**<br>\n",
    "\n",
    "check VPC network for default network<br>\n",
    "and check the firewall rules for ssh<br>\n",
    "\n",
    "go to the Compute Engine and create a new instance<br>\n",
    "name: instance1<br>\n",
    "region: us-east1, us-east1-b<br>\n",
    "machine type: e2 micro<br>\n",
    "service account: Set access for each API: Compute Engine: Read/Write<br>\n",
    "<br>\n",
    "\n",
    "**create a separate persistent disks**<br>\n",
    "\n",
    "Compute Engine -> Disks<br>\n",
    "disks for the instance is there, with 10GB<br>\n",
    "but, click on create a new disk<br>\n",
    "name: newpd<br>\n",
    "region: us-east1, us-east1-b<br>\n",
    "(if you click replicate this disk, you change it to zonal to regional pd)<br>\n",
    "source type: blank disk: size 100GB (look at the est. performance)<br>\n",
    "label: key: env, value: testing<br>\n",
    "<br>\n",
    "\n",
    "**attach to the disk**<br>\n",
    "\n",
    "back to the VM instance and ssh into the instance<br>\n",
    "\n",
    "> lsblk<br>\n",
    "> gcloud compute instances attach-disk instance1 --disk newpd --zone us-east1-b<br>\n",
    "> lsblk //check if it's been attached<br>\n",
    "\n",
    "this is formatting the disk to ext4 format<br>\n",
    "\n",
    "> sudo file -s /dev/sdb<br>\n",
    "> sudo mkfs.ext4 -F /dev/sdb<br>\n",
    "> sudo file -s /dev/sdb //check if it's been formatted<br>\n",
    "\n",
    "mount the disk<br>\n",
    "\n",
    "> sudo mkdir /newpd<br>\n",
    "> sudo mount /dev/sdb /newpd<br>\n",
    "> lsblk<br>\n",
    "\n",
    "sdb has been mounted as a /newpd<br>\n",
    "now we can interact with the disk<br>\n",
    "\n",
    "> cd /newpd/<br>\n",
    "> ls<br>\n",
    "\n",
    "lost+found directory is found in every linux system<br>\n",
    "to place orphan or corrupted files to place here<br>\n",
    "\n",
    "> sudo nano fileoftext.txt<br>\n",
    "\n",
    "type in the text and press Ctrl+O to save<br>\n",
    "Hit enter to verify and hit Ctrl+X to exit<br>\n",
    "\n",
    "> ls<br>\n",
    "> df -k<br>\n",
    "> sudo reboot<br>\n",
    "\n",
    "wait for a minute, before clicking SSH again<br>\n",
    "\n",
    "> lsblk<br>\n",
    "\n",
    "newpd didn't sustain through reboot, due to a config file in linux<br>\n",
    "that points to which partitions mount automatically on startup<br>\n",
    "\n",
    "> sudo blkid /dev/sdb<br>\n",
    "\n",
    "copy the UUID<br>\n",
    "\n",
    "> sudo nano /etc/fstab<br>\n",
    "\n",
    "find UUID for another partition<br>\n",
    "move down the line and add a new line<br>\n",
    "\n",
    "> UUID=[paste the UUID that I just copied] /newpd ext4 defaults,nofail<br>\n",
    "\n",
    "Ctrl+O to save, Enter to verify, Ctrl+X to exit<br>\n",
    "\n",
    "> sudo mount -a<br>\n",
    "> lsblk<br>\n",
    "\n",
    "this is a common task working on linux attaching a disk, and also can be scripted<br>\n",
    "\n",
    "**then resize the disk**<br>\n",
    "\n",
    "100GB -> 150GB<br>\n",
    "go back to disks, click edit<br>\n",
    "150GB and click save<br>\n",
    "\n",
    "in terminal to resize the disk<br>\n",
    "> gcloud compute disks resize newpd --size 150 --zone us-east1-b<br>\n",
    "\n",
    "Remember: you can only make it bigger, NEVER smaller<br>\n",
    "\n",
    "> df -k<br>\n",
    "\n",
    "only displays 100GB, because we didn't allocate the raw blocks to the file systems<br>\n",
    "\n",
    "> sudo resize2fs /dev/sdb<br>\n",
    "> df -k<br>\n",
    "\n",
    "> ls -al<br>\n",
    "> ls<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "**deleting the disk and clean up**<br>\n",
    "\n",
    "detach the disk<br>\n",
    "> gcloud compute instances detach-disk instance1 --disk newpd --zone us-east1-b<br>\n",
    "\n",
    "head back to the console and delete the disk<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
